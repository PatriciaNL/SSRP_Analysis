{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main objective is to plot heatmap of nandata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np \n",
    "import scipy.io\n",
    "from scipy.io   import  loadmat\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt #import matplotlib as plt\n",
    "from scipy.optimize import curve_fit \n",
    "import seaborn as sns #import mat73\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumBins = 6\n",
    "NumHarms = 4\n",
    "rca_comp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avilable Files to choose from: 5\n",
      "Files on hand: ['desktop.ini', 'OfficialF1_Control_rcaResults_Sweep.mat', 'OfficialF1_rcaResults_Sweep_F2F4F6F8.mat', 'OfficialF2_Control_rcaResults_Sweep.mat', 'OfficialF2_rcaResults_Sweep_F2F4F6F8.mat']\n",
      "Current WD: C:\\plimon\\LTP_analysis\\RCA_Fx_OutputData\\RCA_Files\\OfficialF1_rcaResults_Sweep_F2F4F6F8.mat\n",
      "Does File #1 Exist? True\n",
      "Current WD: C:\\plimon\\LTP_analysis\\RCA_Fx_OutputData\\RCA_Files\\OfficialF2_rcaResults_Sweep_F2F4F6F8.mat\n",
      "Does File #2 Exist? True\n"
     ]
    }
   ],
   "source": [
    "# Main Directory of processed file from MatLab\n",
    "MainDir = 'C:\\\\plimon\\\\LTP_analysis\\\\RCA_Fx_OutputData\\\\RCA_Files\\\\' # set dir - on my computer\n",
    "os.chdir(MainDir) # change old dir, to this dir\n",
    "d = os.listdir(MainDir) # list files in dir\n",
    "print(f'Avilable Files to choose from: {len(d)}')\n",
    "print(f'Files on hand: {d}')\n",
    "##############################################\n",
    "FileN_f1 = d[2]#d[1] # choose one\n",
    "FileN_f2  = d[4]# d[3]                         \n",
    "file_path1 = os.path.join(MainDir, FileN_f1) # join paths and prep 2 load\n",
    "print('Current WD:',file_path1) # does path exist ... ?\n",
    "print('Does File #1 Exist?',os.path.exists(file_path1)) # yes or no\n",
    "\n",
    "file_path2 = os.path.join(MainDir, FileN_f2) # join paths and prep 2 load\n",
    "print('Current WD:',file_path2) # does path exist ... ?\n",
    "print('Does File #2 Exist?',os.path.exists(file_path1)) # yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the base colors for blending\n",
    "base_colors = ['black','green', 'blue', 'pink', 'purple', 'orange', 'pink']\n",
    "# Number of colors needed in the spectrum\n",
    "num_colors = 18\n",
    "fill = sns.blend_palette(base_colors, n_colors=num_colors, as_cmap=False)\n",
    "sns.palplot(fill)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1 = scipy.io.loadmat(file_path1)\n",
    "df_f2 = scipy.io.loadmat(file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_f1 = df_f1['rcaResult']['projectedData'][0,0]\n",
    "f1 = [rca_f1[x,0] for x in range(rca_f1.shape[0])] # entry per subject\n",
    "\n",
    "F1_noise = df_f1['rcaResult']['noiseData'][0,0]\n",
    "noise_f1 = [F1_noise[x,0] for x in range(F1_noise.shape[0])][0][1]# entry per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_f2 = df_f2['rcaResult']['projectedData'][0,0]\n",
    "f2 = [rca_f2[x,0] for x in range(rca_f2.shape[0])]\n",
    "\n",
    "F2_noise = df_f2['rcaResult']['noiseData'][0,0]\n",
    "noise_f2 = [F2_noise[x,0] for x in range(F2_noise.shape[0])][0][1]# entry per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Files: 66\n"
     ]
    }
   ],
   "source": [
    "# load subject names ...\n",
    "SubNames = df_f1['rcaResult'][0,0][5]\n",
    "FileName = [x[0][3:] for subjlist in SubNames for x in subjlist[0][2][0]] # rm 'nl-'\n",
    "#FileName = [x[0][3:] for subjlist in SubNames for x in subjlist[0][2][0]]\n",
    "UniformFileNames = [str(s).replace('-', '_') for s in FileName]\n",
    "print(f'Total Data Files: {len(FileName)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "NumFiles = int(len(FileName))\n",
    "print(NumFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "File_Split_CatchString = '_'\n",
    "SubjectNum_Array = [x.split(File_Split_CatchString)[0] for x in UniformFileNames]\n",
    "SubjExpCo = [x.split(File_Split_CatchString)[1] for x in UniformFileNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all participants: 38\n",
      "Complete Files: 28.0\n",
      "Single Session Files: 10\n"
     ]
    }
   ],
   "source": [
    "AllSubject_IDs, SessionsCompleted = np.unique(np.array(SubjectNum_Array), return_counts = True)\n",
    "\n",
    "print(f'all participants: {len(AllSubject_IDs)}')\n",
    "\n",
    "print(f'Complete Files: {np.sum(SessionsCompleted[SessionsCompleted == 2])/2}')\n",
    "print(f'Single Session Files: {np.sum(SessionsCompleted[SessionsCompleted == 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_As_Average2(rca_comp, NumHarms, NumBins,NoiseFileIn): # for avg signal data\n",
    "    NumSubs = int(len(NoiseFileIn))\n",
    "    ProcessedNoiseOut = np.zeros((NumSubs,NumHarms,NumBins))\n",
    "\n",
    "    AvgCRFSignal = []\n",
    "    nanData = []\n",
    "\n",
    "    for Sub in range(NumSubs):\n",
    "        noise = np.array(NoiseFileIn[Sub][:,rca_comp,:])\n",
    "        noise[noise == 0] = np.nan\n",
    "\n",
    "        nanData.append(noise)\n",
    "\n",
    "        r,c = noise.shape\n",
    "        harmoniSegments = np.arange(0,int(c/2),NumBins)\n",
    "        realdf = noise[0:int(r/2)] # [24 x 78 ]\n",
    "        imaginarydf = noise[int(r/2):,:]\n",
    "        Noise = np.zeros((NumHarms,NumBins))\n",
    "        for harm_noise in range(NumHarms):\n",
    "            s = harmoniSegments[harm_noise]\n",
    "            e = s + NumBins\n",
    "            reals = np.nanmean(realdf[s:e,:],axis = 1)\n",
    "            imag = np.nanmean(imaginarydf[s:e,:],axis = 1)\n",
    "            Noise[harm_noise,:] = np.hypot(reals,imag)\n",
    "        ProcessedNoiseOut[Sub,:,:] = Noise\n",
    "\n",
    "        AvgCRFSignal.append(ProcessedNoiseOut)\n",
    "    return AvgCRFSignal, nanData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoiseFiles = [noise_f1, noise_f2]\n",
    "SignalFiles = [f1, f2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average signal across pre and post induction\n",
    "F1_AverageCRF, rawF1 = Sort_As_Average2(rca_comp, NumHarms, NumBins,NoiseFileIn = SignalFiles[0])\n",
    "F2_AverageCRF,rawF2 = Sort_As_Average2(rca_comp, NumHarms, NumBins,NoiseFileIn = SignalFiles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "(48, 78)\n"
     ]
    }
   ],
   "source": [
    "print(len(rawF1))\n",
    "print((rawF1[0].shape)) # complex data stored for 1 subj: 48 rows 1/2 real 1/2 imagnary(6 per harmonic) per trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConcateComplexValues_Horizont(isNaN_Signal_Data, NumBins, NumHarms):\n",
    "    \"\"\"This function segments complex values per harmonic side by side for one rca component only\n",
    "     RETURNS: UNPROCESSED DATA BUT READY FOR HARMONIC SEGMENTATION \"\"\"\n",
    "    dims = isNaN_Signal_Data.shape # import processed data [48 x 78]\n",
    "    sigData = isNaN_Signal_Data\n",
    "    ComplexVerticalEnd = int(dims[0]/2) # 24th row is where imaginary values are stored\n",
    "    ComplexHarmonicSort = np.concatenate((sigData[0:ComplexVerticalEnd,:],sigData[ComplexVerticalEnd:,:]),axis= 1) # concate real and imaginary data vertically\n",
    "    \n",
    "    return ComplexHarmonicSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComplexDataReshape = []\n",
    "\n",
    "for s in range(len(SignalFiles[0])):\n",
    "    temp = ConcateComplexValues_Horizont(isNaN_Signal_Data = rawF1[s], NumBins = 6, NumHarms = 4)\n",
    "    #print(temp.shape) # [24 x ~150]\n",
    "    ComplexDataReshape.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "(24, 156)\n"
     ]
    }
   ],
   "source": [
    "print(len(ComplexDataReshape)) # 66 arrays\n",
    "print(ComplexDataReshape[0].shape) # 24 rows (6 per harmonic 0-6 sweep) x 1/2 cols are real and last 1/2 are complex values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 156)\n",
      "10.256410256410255\n"
     ]
    }
   ],
   "source": [
    "# sweepDataIn = ComplexDataReshape[0]#[:,:]\n",
    "# print(sweepDataIn.shape)\n",
    "# dataSize = np.size(sweepDataIn)\n",
    "# nanCounts = (np.sum(np.isnan(sweepDataIn).astype(int)))\n",
    "# nanFrequency = (nanCounts / dataSize) * 100\n",
    "\n",
    "# print(nanFrequency)\n",
    "\n",
    "# test = np.array(ComplexDataReshape[0]) #  [24 x 156]\n",
    "# print(test.shape)\n",
    "\n",
    "# NaN_CounterCentile = np.zeros((NumHarms,NumBins)) # [4 x 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "#testbinEpochs = []\n",
    "PickHarmInd = 1 # doesnt really make a difference, but for faster computing just use 1st harmonic (2F)\n",
    "NaN_CounterCentile = np.zeros((PickHarmInd,NumBins)) # [Number of harmonics to keep x 6]\n",
    "print(NaN_CounterCentile.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "def Count_NaNsIn_Signal(SingalImp = ComplexDataReshape[0], PickHarmInd = 1, NumBins = 6):\n",
    "    for h in range(PickHarmInd): # iterate for 1 harmonic signal data \n",
    "        dataIn = SingalImp[(h*6):((h+1)*6),:] # [6 x 156 [78 x 78]] \n",
    "        #print(dataIn.shape)\n",
    "        for c in range (NumBins):\n",
    "            sweepDataIn = dataIn[c,:] # 156 real and imaginary for 1 contrast \n",
    "            dataSize = np.size(sweepDataIn)\n",
    "            nanCounts = (np.sum(np.isnan(sweepDataIn).astype(int)))\n",
    "            nanFrequency = (nanCounts / dataSize) * 100 # divide nan counts by total data aquired + convert to percentage\n",
    "            #print(nanCounts,nanFrequency) # ti see nan totals and as a percentage\n",
    "            NaN_CounterCentile[h,c] = nanFrequency # save score to visualize later \n",
    "            return NaN_CounterCentile # [x * 6] array of nan perceptil score per bin\n",
    "\n",
    "#print(NaN_CounterCentile)\n",
    "#def NaNCounts_PerSubjHemiF(ReshapedSignal = ComplexDataReshape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(figsize = (7,1))\n",
    "sns.heatmap(NaN_CounterCentile, annot=True, cmap='bwr', cbar=True, yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for h in range(6):\n",
    "# HarmonicProcess = complex_HarmonicSort[0:numBins,:]\n",
    "# print(HarmonicProcess.shape)\n",
    "NaN_Count_Score = np.zeros((1,6))\n",
    "print(NaN_Count_Score.shape)\n",
    "for c in range(numBins):\n",
    "    dataIn = complex_HarmonicSort[c,:]\n",
    "    dataSize = np.size(dataIn)\n",
    "    nanCounts = (np.sum(np.isnan(dataIn).astype(int)))\n",
    "    # # print(dataIn.shape)\n",
    "    # print(dataSize,nanCounts)\n",
    "    nanFrequency = (nanCounts / dataSize) * 100\n",
    "\n",
    "    NaN_Count_Score[0,c] = nanFrequency\n",
    "\n",
    "print(NaN_Count_Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_Count_Score = np.array(NaN_Count_Score)\n",
    "fig,axs = plt.subplots(figsize = (7,1))\n",
    "sns.heatmap(NaN_Count_Score, annot=True, cmap='bwr', cbar=True, yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_Count_Score = []\n",
    "for h in range(4):\n",
    "    #print(f'start {h*6}, end {(h+1)*6}')\n",
    "    dataIn = complex_HarmonicSort[(h*6):((h+1)*6),:]\n",
    "    dataSize = np.size(dataIn)\n",
    "    nanCounts = (np.sum(np.isnan(dataIn).astype(int)))\n",
    "    # print(dataIn.shape)\n",
    "    print(dataSize)\n",
    "    print(nanCounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(F1_AverageCRF))\n",
    "print(F1_AverageCRF[0].shape)\n",
    "\n",
    "for h in range(4):\n",
    "    plt.plot(F1_AverageCRF[0][h,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
